---
title: 'Data Wrangling in \texttt{R}'
author: "Jerry Xin STA 360: Homework 2"
date: "Due Friday September 3rd, 5 PM EDT"
output: pdf_document
---

```{r setup, echo = F}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center")
```

```{r load-packages, warning = F, message = F}
library(tidyverse)
library(knitr)
library(broom)
```

1. Lab component (30 points total) Please refer to lab 2 and complete tasks
3â€”5.
(a) (10) Task 3
(b) (10) Task 4
(c) (10) Task 5

```{r,echo=TRUE}
# set a seed
set.seed(123)
# create the observed data
obs.data <- rbinom(n = 100, size = 1, prob = 0.01)
# inspect the observed data
head(obs.data)
tail(obs.data)
length(obs.data)
```

# Task 3

Write a function that takes as its inputs that data you simulated (or any data of the same type) and a sequence of $\theta$ values of length 1000 and produces Likelihood values based on the Binomial Likelihood. Plot your sequence and its corresponding Likelihood function.

The likelihood function is given below. Since this is a probability and is only valid over the interval from $[0, 1]$ we generate a sequence over that interval of length 1000.

You have a rough sketch of what you should do for this part of the assignment. Try this out in lab on your own. 

```{r, echo = TRUE}
### Bernoulli LH Function ###

# Input: obs.data, theta
# Output: bernoulli likelihood

myBernLH <- function(obs.data, theta){
  N <- length(obs.data)
  x <- sum(obs.data)
  LH <- (theta^x) * ((1-theta)^(N-x))
  return(LH)
}


### Plot LH for a grid of theta values ###
# Create the grid #
# Store the LH values
# Create the Plot
theta.sim <- seq(from = 0, to = 1, length.out = 1000)
sim.LH <- myBernLH(obs.data, theta = theta.sim)
plot(theta.sim, sim.LH, type ="line", main = "Likelihood Profile", xlab = "Simulated Support", ylab = "Likelihood")
```
# Task 4 (To be completed for homework)

Write a function that takes as its inputs  prior parameters \textsf{a} and \textsf{b} for the Beta-Bernoulli model and the observed data, and produces the posterior parameters you need for the model. \textbf{Generate and print} the posterior parameters for a non-informative prior i.e. \textsf{(a,b) = (1,1)} and for an informative case \textsf{(a,b) = (3,1)}}.

```{r}
posterior <- function(priorA, priorB, obs.data)
{
  N <- length(obs.data)
  x <- sum(obs.data)
  postA <- priorA + x
	postB <- priorB + N - x
	postparam <- list('postA' = postA, 
	 'postB' = postB)
	return(postparam)
}
```

```{r}
nonInformative <- posterior(priorA = 1, priorB = 1, obs.data = obs.data)
informative <- posterior(priorA = 3, priorB = 1, obs.data = obs.data)
print(c(nonInformative, informative))
```

# Task 5 (To be completed for homework)

Create two plots, one for the informative and one for the non-informative case to show the posterior distribution and superimpose the prior distributions on each along with the likelihood. What do you see? Remember to turn the y-axis ticks off since superimposing may make the scale non-sense.
```{r}
nonInformativeDen <- dbeta(x = theta.sim, shape1 = nonInformative$postA, 
 shape2 =nonInformative$postB)
informativeDen <- dbeta(x = theta.sim, shape1 = informative$postA, 
 shape2 = informative$postB)
priorInform <- dbeta(x = theta.sim, shape1 = 3, 
 shape2 = 1)
priorNonInform <- dbeta(x = theta.sim, shape1 = 1, 
 shape2 = 1)

par(mfrow=c(1, 2)) 
plot(theta.sim, sim.LH, lty = 2, xlab = 'Simulated Thetas',
 ylab = 'Density/LH', type = 'l', yaxt = 'n', main = 'Informative',
 , col = c("red"))
par(new = TRUE)
plot(theta.sim, informativeDen, lty = 1, axes = FALSE, xlab = '', ylab = '',
 type = 'l', , col = c("green"))
par(new = TRUE)
plot(theta.sim, priorInform, lty = 3, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("blue"))
legend('topright', lty=c(2,1,3), legend = c('LH', 'Posterior', 'Prior'),
       col = c("red", "green", "blue"), cex = 0.5)

plot(theta.sim, sim.LH, lty = 2, xlab = 'Simulated Thetas',
 ylab = 'Density/LH', type = 'l', yaxt = 'n', main = 'Non-informative', 
 col = c("red"))
par(new = TRUE)
plot(theta.sim, nonInformativeDen, lty = 1, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("green"))
par(new = TRUE)
plot(theta.sim, priorNonInform, lty = 3, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("blue"))
legend('topright', lty=c(2,1,3), legend = c('LH', 'Posterior', 'Prior'), 
       col = c("red", "green", "blue"), cex=0.5)

```


These graphs both show the posterior distribution is a result of combining the Likelihood and the Prior. It is graphically seen as an average between the Likelihood and Prior. A non-informative prior is graphically seen as a flat line and seems to have less effect on the posterior than a prior which is informative, which is graphically a upwards sloping curve, and is shown to have a larger effect on moving the posterior rightwards.

\item (20  points total) {\em The Exponential-Gamma Model}
We write $X\sim\Exp(\theta)$ to indicate that $X$ has the Exponential distribution, that is, its p.d.f.\ is
$$ p(x|\theta) =\Exp(x|\theta) = \theta\exp(-\theta x)\I(x>0). $$
The Exponential distribution has some special properties that make it a good model for certain applications. It has been used to model the time between events (such as neuron spikes, website hits, neutrinos captured in a detector), extreme values such as maximum daily rainfall over a period of one year, or the amount of time until a product fails (lightbulbs are a standard example).

Suppose you have data $x_1,\dotsc,x_n$ which you are modeling as i.i.d.\ observations from an Exponential distribution, and suppose that your prior is $\btheta\sim\Ga(a,b)$, that is,
$$ p(\theta) = \Ga(\theta|a,b) = \frac{b^a}{\Gamma(a)}\theta^{a-1}\exp(-b\theta) \I(\theta>0). $$

\begin{enumerate}
\item (5) Derive the formula for the posterior density, $p(\theta|x_{1:n})$. Give the form of the posterior in terms of one of the most common distributions (Bernoulli, Beta, Exponential, or Gamma).

$$\theta\exp(-\theta x)\theta^{a-1}\exp(-b\theta)$$

$$\theta^{a}\exp(-b\theta x)$$
$$\Ga(\theta|a+1,bx)$$
The posterior is a Gamma Distribution with Gamma(a+1, bx).

\item (5) Why is the posterior distribution a \emph{proper} density or probability distribution function? \
The posterior distribution is a proper density, or probability distribution function, because it integrates to 1, which means the area under the function adds up to 1.

\item (5) Now, suppose you are measuring the number of seconds between lightning strikes during a storm, your prior is $\Ga(0.1,1.0)$, and your data is
$$(x_1,\dotsc,x_8) = (20.9, 69.7, 3.6, 21.8, 21.4, 0.4, 6.7, 10.0).$$
Plot the prior and posterior p.d.f.s. (Be sure to make your plots on a scale that allows you to clearly see the important features.)



\item (5) Give a specific example of an application where an Exponential model would be reasonable. Give an example where an Exponential model would NOT be appropriate, and explain why.
\end{enumerate}

\item (40 points total) {\em Priors, Posteriors, Predictive Distributions (Hoff, 3.9)}
An unknown quantity $Y$ has a Galenshore($a, \theta$) distribution if its density is given by 
$$p(y) = \frac{2}{\Gamma(a)} \; \theta^{2a} y^{2a - 1} e^{-\theta^2 y^2}$$
for $y>0, \theta >0, a>0.$ Assume for now that $a$ is known. For this density, 
$$E[Y] = \frac{\Gamma(a +1/2)}{\theta \Gamma(a)}$$ and 
$$E[Y^2] = \frac{a}{\theta^2}.$$

\begin{enumerate}
\item (10) Identify a class of conjugate prior densities for $\theta$. \textcolor{red}{Assume the prior parameters are $c$ and $d.$} Plot a few members of this class of densities.
\item (5) Let $Y_1, \ldots, Y_n \stackrel{iid}{\sim}$ Galenshore($a, \theta$). Find the posterior distribution of $\theta \mid y_{1:n}$ using a prior from your conjugate class. 
\item (10) Write down $$\frac{p(\theta_a \mid y_{1:n})}{p(\theta_b \mid y_{1:n})}$$ and simplify. Identify a sufficient statistic. 
\item (5) Determine $E[\theta \mid y_{1:n}]$.
\item (10) Show that the form of the posterior predictive density $$p(y_{n+1} \mid y_{1:n}) =  \frac{2 y_{n+1}^{2a - 1} \Gamma(an + a + c)}{\Gamma(a)\Gamma(an + c)}
\frac{(d^2 + \sum y_i^2)^{an + c}}{(d^2 + \sum y_i^2 + y_{n+1}^2)^{(an + a + c)}}.$$