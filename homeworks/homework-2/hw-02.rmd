---
title: 'Data Wrangling in \texttt{R}'
author: "Jerry Xin STA 360: Homework 2"
date: "Due Friday September 3rd, 5 PM EDT"
output: pdf_document
---

```{r setup, echo = F}
knitr::opts_chunk$set(warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center")
```

```{r load-packages, warning = F, message = F}
library(tidyverse)
library(knitr)
library(broom)
```

1. Lab component (30 points total) Please refer to lab 2 and complete tasks
3â€”5.
(a) (10) Task 3
(b) (10) Task 4
(c) (10) Task 5

```{r,echo=TRUE}
# set a seed
set.seed(123)
# create the observed data
obs.data <- rbinom(n = 100, size = 1, prob = 0.01)
# inspect the observed data
head(obs.data)
tail(obs.data)
length(obs.data)
```

# Task 3

Write a function that takes as its inputs that data you simulated (or any data of the same type) and a sequence of $\theta$ values of length 1000 and produces Likelihood values based on the Binomial Likelihood. Plot your sequence and its corresponding Likelihood function.

The likelihood function is given below. Since this is a probability and is only valid over the interval from $[0, 1]$ we generate a sequence over that interval of length 1000.

You have a rough sketch of what you should do for this part of the assignment. Try this out in lab on your own. 

```{r, echo = TRUE}
### Bernoulli LH Function ###

# Input: obs.data, theta
# Output: bernoulli likelihood

myBernLH <- function(obs.data, theta){
  N <- length(obs.data)
  x <- sum(obs.data)
  LH <- (theta^x) * ((1-theta)^(N-x))
  return(LH)
}


### Plot LH for a grid of theta values ###
# Create the grid #
# Store the LH values
# Create the Plot
theta.sim <- seq(from = 0, to = 1, length.out = 1000)
sim.LH <- myBernLH(obs.data, theta = theta.sim)
plot(theta.sim, sim.LH, type ="line", main = "Likelihood Profile", xlab = "Simulated Support", ylab = "Likelihood")
```
# Task 4 (To be completed for homework)

Write a function that takes as its inputs  prior parameters \textsf{a} and \textsf{b} for the Beta-Bernoulli model and the observed data, and produces the posterior parameters you need for the model. \textbf{Generate and print} the posterior parameters for a non-informative prior i.e. \textsf{(a,b) = (1,1)} and for an informative case \textsf{(a,b) = (3,1)}}.

```{r}
posterior <- function(priorA, priorB, obs.data)
{
  N <- length(obs.data)
  x <- sum(obs.data)
  postA <- priorA + x
	postB <- priorB + N - x
	postparam <- list('postA' = postA, 
	 'postB' = postB)
	return(postparam)
}
```

```{r}
nonInformative <- posterior(priorA = 1, priorB = 1, obs.data = obs.data)
informative <- posterior(priorA = 3, priorB = 1, obs.data = obs.data)
print(c(nonInformative, informative))
```

# Task 5 (To be completed for homework)

Create two plots, one for the informative and one for the non-informative case to show the posterior distribution and superimpose the prior distributions on each along with the likelihood. What do you see? Remember to turn the y-axis ticks off since superimposing may make the scale non-sense.
```{r}
nonInformativeDen <- dbeta(x = theta.sim, shape1 = nonInformative$postA, 
 shape2 =nonInformative$postB)
informativeDen <- dbeta(x = theta.sim, shape1 = informative$postA, 
 shape2 = informative$postB)
priorInform <- dbeta(x = theta.sim, shape1 = 3, 
 shape2 = 1)
priorNonInform <- dbeta(x = theta.sim, shape1 = 1, 
 shape2 = 1)

par(mfrow=c(1, 2)) 
plot(theta.sim, sim.LH, lty = 2, xlab = 'Simulated Thetas',
 ylab = 'Density/LH', type = 'l', yaxt = 'n', main = 'Informative',
 , col = c("red"))
par(new = TRUE)
plot(theta.sim, informativeDen, lty = 1, axes = FALSE, xlab = '', ylab = '',
 type = 'l', , col = c("green"))
par(new = TRUE)
plot(theta.sim, priorInform, lty = 3, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("blue"))
legend('topright', lty=c(2,1,3), legend = c('LH', 'Posterior', 'Prior'),
       col = c("red", "green", "blue"), cex = 0.5)

plot(theta.sim, sim.LH, lty = 2, xlab = 'Simulated Thetas',
 ylab = 'Density/LH', type = 'l', yaxt = 'n', main = 'Non-informative', 
 col = c("red"))
par(new = TRUE)
plot(theta.sim, nonInformativeDen, lty = 1, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("green"))
par(new = TRUE)
plot(theta.sim, priorNonInform, lty = 3, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("blue"))
legend('topright', lty=c(2,1,3), legend = c('LH', 'Posterior', 'Prior'), 
       col = c("red", "green", "blue"), cex=0.5)

```


These graphs both show the posterior distribution is a result of combining the Likelihood and the Prior. It is graphically seen as an average between the Likelihood and Prior. A non-informative prior is graphically seen as a flat line and seems to have less effect on the posterior than a prior which is informative, which is graphically a upwards sloping curve, and is shown to have a larger effect on moving the posterior rightwards.

\item (20  points total) {\em The Exponential-Gamma Model}
We write $X\sim\Exp(\theta)$ to indicate that $X$ has the Exponential distribution, that is, its p.d.f.\ is
$$ p(x|\theta) =\Exp(x|\theta) = \theta\exp(-\theta x)\I(x>0). $$
The Exponential distribution has some special properties that make it a good model for certain applications. It has been used to model the time between events (such as neuron spikes, website hits, neutrinos captured in a detector), extreme values such as maximum daily rainfall over a period of one year, or the amount of time until a product fails (lightbulbs are a standard example).

Suppose you have data $x_1,\dotsc,x_n$ which you are modeling as i.i.d.\ observations from an Exponential distribution, and suppose that your prior is $\btheta\sim\Ga(a,b)$, that is,
$$ p(\theta) = \Ga(\theta|a,b) = \frac{b^a}{\Gamma(a)}\theta^{a-1}\exp(-b\theta) \I(\theta>0). $$

\begin{enumerate}
\item (5) Derive the formula for the posterior density, $p(\theta|x_{1:n})$. Give the form of the posterior in terms of one of the most common distributions (Bernoulli, Beta, Exponential, or Gamma).

$$\theta\exp(-\theta x)\theta^{a-1}\exp(-b\theta)$$

$$\theta^{a}\exp(-\theta *(b + x))$$
$$\Ga(\theta|a+1,b + x)$$
The posterior is a Gamma Distribution with Gamma(a+1, bx).

\item (5) Why is the posterior distribution a \emph{proper} density or probability distribution function? \
The posterior distribution is a proper density, or probability distribution function, because it integrates to 1, which means the area under the function adds up to 1.

\item (5) Now, suppose you are measuring the number of seconds between lightning strikes during a storm, your prior is $\Ga(0.1,1.0)$, and your data is
$$(x_1,\dotsc,x_8) = (20.9, 69.7, 3.6, 21.8, 21.4, 0.4, 6.7, 10.0).$$
Plot the prior and posterior p.d.f.s. (Be sure to make your plots on a scale that allows you to clearly see the important features.)

```{r,echo=TRUE}
# set a seed
set.seed(123)
# create the observed data
obs.data2 <- c(20.9, 69.7, 3.6, 21.8, 21.4, 0.4, 6.7, 10.0)
# inspect the observed data
head(obs.data2)
tail(obs.data2)
length(obs.data2)
```
```{r}
posterior2 <- function(priorA, priorB, obs.data)
{
  N <- length(obs.data)
  x <- sum(obs.data)
  postA <- priorA + 1
	postB <- priorB + x
	postparam <- list('postA' = postA, 
	 'postB' = postB)
	return(postparam)
}
```

```{r}
informative1<- posterior2(priorA = 0.1, priorB = 1.0, obs.data = obs.data)
print(c(informative))
```
```{r}
informativeDen1 <- dgamma(x = theta.sim, shape = informative1$postA, 
 rate = informative1$postB)
priorInform1 <- dgamma(x = theta.sim, shape = 0.1, 
 rate = 1)

plot(theta.sim, informativeDen, lty = 1, axes = FALSE, xlab = '', ylab = '',
 type = 'l', , col = c("green"))
par(new = TRUE)
plot(theta.sim, priorInform, lty = 3, axes = FALSE, xlab = '', ylab = '',
 type = 'l', col = c("blue"))
legend('topright', lty=c(2,1,3), legend = c('LH', 'Posterior', 'Prior'),
       col = c("red", "green", "blue"), cex = 0.5)
```


\item (5) Give a specific example of an application where an Exponential model would be reasonable. Give an example where an Exponential model would NOT be appropriate, and explain why.
\end{enumerate}

Exponential functions assuming a constant multiplicative rate of change, so a specific application would be studying a bacteria in a petri dish that undergoes binary fission(splits into 2 identical replicas). The constant multiplicative rate of change here would be 2, and would not change from 2 ever. An example where the exponential model would not be appropiate is modeling the stock market, as there is not anywhere close to a constant rate of change in the stock market, which is highly unpredictable and consists of lots of valleys and peaks, not to mention recessions and economic stagnation. There is no constant multiplicative rate of change here.

Constant Rate Assumption with the Exponential Model, time in between events. Assuming constant time in between events.

\item (40 points total) {\em Priors, Posteriors, Predictive Distributions (Hoff, 3.9)}
An unknown quantity $Y$ has a Galenshore($a, \theta$) distribution if its density is given by 
$$p(y) = \frac{2}{\Gamma(a)} \; \theta^{2a} y^{2a - 1} e^{-\theta^2 y^2}$$
for $y>0, \theta >0, a>0.$ Assume for now that $a$ is known. For this density, 
$$E[Y] = \frac{\Gamma(a +1/2)}{\theta \Gamma(a)}$$ and 
$$E[Y^2] = \frac{a}{\theta^2}.$$

\begin{enumerate}
\item (10) Identify a class of conjugate prior densities for $\theta$. \textcolor{red}{Assume the prior parameters are $c$ and $d.$} Plot a few members of this class of densities.

Let the prior also be a Galenshore($c, $d)  distribution:
$$p(\theta) = \frac{2}{\Gamma(c)} \; d^{2c} \theta^{2c - 1} e^{-d^2 \theta^2}$$
The likelihood a Galenshore($a, \theta$)  distribution:
$$p(y) = \frac{2}{\Gamma(a)} \; \theta^{2a} y^{2a - 1} e^{-\theta^2 y^2}$$
Because you have $\theta^{2a}$ , and $e^{-\theta^2 y^2}$, only a Galenshore distribution would work as a conjugate prior density.
```{r}
dgalenshore <- function(theta, c, d) {
    # takes numeric vector (elements should be >0) and returns the galenshore
    # probability density at each point. Parameters a and theta should all be
    # > 0.
    if (any(theta <= 0) || any(c <= 0) || any(d <= 0)) {
        stop("Invalid parameters or input values. Inputs must be >0")
    } else {
        return((2/gamma(c)) * d^(2 * c) * exp(-d^2 * theta^2))
    }
}
x <- seq(0.01, 3, 0.01)
plot(x, dgalenshore(x, 1, 1), type = "l", ylab = "Galenshore Density(x)", col = "black", 
    ylim = c(0, 5), lwd = 2)
lines(x, dgalenshore(x, 1, 1.5), type = "l", col = "red", lwd = 2, lty = 2)
lines(x, dgalenshore(x, 3, 1), type = "l", col = "blue", lwd = 2, lty = 3)
legend("topright", c("Galenshore(1,1)", "Galenshore(1,2)", "Galenshore(3,1)"), 
    col = c("black", "red", "blue"), lwd = c(2, 2, 2), lty = c(1, 2, 3))
```

\item (5) Let $Y_1, \ldots, Y_n \stackrel{iid}{\sim}$ Galenshore($a, \theta$). Find the posterior distribution of $\theta \mid y_{1:n}$ using a prior from your conjugate class. 
You have a sample of ys, $y_{1:n}$

\item (10) Write down $$\frac{p(\theta_a \mid y_{1:n})}{p(\theta_b \mid y_{1:n})}$$ and simplify. Identify a sufficient statistic. 
Compared the ratio of 2 different likelihood distributions, one has parameter a and one has parameter b. Normalizing constants will cancel out, some simplification.

\item (5) Determine $E[\theta \mid y_{1:n}]$.
\item (10) Show that the form of the posterior predictive density $$p(y_{n+1} \mid y_{1:n}) =  \frac{2 y_{n+1}^{2a - 1} \Gamma(an + a + c)}{\Gamma(a)\Gamma(an + c)}
\frac{(d^2 + \sum y_i^2)^{an + c}}{(d^2 + \sum y_i^2 + y_{n+1}^2)^{(an + a + c)}}.$$